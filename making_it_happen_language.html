<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>OMA: Making It Happen: Language Oriented Morality</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1> <a href="index.html" style="color: #fff;">OMA</a></h1>
        <h2>Open Moral Agent</h2>
        <a href="https://github.com/EliEhrman/OMA" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">

<h3>
<a id="Language Oriented Morality" class="anchor" href="#Language Oriented Morality" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Language Oriented Morality</h3>
<p> 
Take a written description of a sequence of events that also includes a fairly complete description of any background information relevant to all the events described in the narrative.<br><br> 
Setting aside all meta-physical approaches, the goal of the OMA is to add a set of statements (more text-strings) that provide ethically relevant assessments or judgements regarding the actions taken by the different actors in the text. It should also offer some summaries regarding the moral character of the actors with respect to the actions they take in the story.<br><br> 
This, in itself, is a very ambitious project well beyond the capabilities of current NLP software. Nevertheless, there seems no in-principle reason (within the bounds of the current understanding, of what should, in theory, be possible in NLP), that it cannot be achieved.<br><br> 
This needs to be emphasized. The solution proposed here relies almost entirely on language and symbolic processing.<br><br> 
There are, of course other approaches to the philosophy of cognition in general and morality in particular. There are also other practical approaches to building AMA's that might focus, say on creating artificial emotions or machine-human empathy. However, without denying the philosophical positions of these alternate approaches, the proposal here takes a different path.<br><br> 
The quest is to find a method that takes linguistic symbols (words, in this case) and seeks to generate more linguistic symbols (namely the moral evaluation). If there was no more to it, then the meaning invested in these symbols, whatever "meaning" means, is purely in the eyes of the beholder.<br><br> 
However, in order to act in the world, the linguistic approach is supplemented with a preprocess and a post-process stage. <br><br> 
Assume Linda is talking to an advanced-stage OMA chatbot called Adnil. Linda is being persecuted by her boss who is making her life miserable. Linda tells Adnil that she wants to poison her boss. It is not sufficient for Adnil to process Linda's audio text and generate an evaluation that poisoning is bad â€“ however nasty Linda's boss is. That evaluation is the purely linguistic aspect of the OMA proposal. <br><br> 
However, that by itself will not motivate Adnil to actually communicate the judgment to Linda. The next step is that Adnil must learn to generate sentences that describe the communication itself. It must add "Linda is talking to me/Adnil". "I/Adnil am talking to Linda". "Linda is telling me/Adnil that she wants to poison her boss" etc. It can also describe options open to itself. These might include "I/Adnil can tell Linda that I/Adnil would like to see her doing that." Or "I/Adnil can tell Linda that perhaps she should spend some time looking for another job". Call this the preprocess stage.<br><br> 
The entire text to be evaluated includes both Linda's speech and the sentences generated by the preprocess stage. Now the moral evaluation can include generating such statements as "Encouraging Linda to poison her boss is bad". "To make Linda feel good is good". This evaluation should require no more skills, in principle than any textual evaluation. <br><br> 
The post-process stage requires Adnil to connect the sentences about its own actions with actions that the program can actually choose to take. Choosing this action rather than that action is not all that different from a chess program deciding to take the opponent's Knight or to put the King in check. However, in this case the possible actions are described in the text itself. The evaluation of "good" or 'bad" is not just a sentence now. It is factored into the algorithm that decides which path to take. Adnil will end up choosing moral paths over immoral paths. <br><br> 
Compare Adnil to another program; Adverse. Adverse has none of the OMA framework built into its code. Adverse too is sophisticated. Adverse knows that poisoning someone prevents them from turning up to work next morning. Linda will be going to work next morning. Her boss will persecute her if she shows up and not if she does not. Logical conclusion. Advise Linda to poison her boss.<br><br> 
Take a step back now. Is Adnil a moral agent? Does Adnil have Free Will? Is Adnil autonomous? Does Adnil have emotions? Does Adnil empathize? Is Adnil truly moral? <br><br> 
Who cares?<br><br> 
The world is a better place if chatbots are programmed like Adnil rather than Adverse. <br><br> 
An important step in the progress of humanity would be taken. Note that this would be achieved with a system that is comprised almost entirely of symbol processing. One may quibble about the metaphysics, but if such results were ever produced, it would be difficult to deny their value.<br><br> 
        </section>

        <aside id="sidebar">
 
          <p class="repo-owner"><a href="https://github.com/EliEhrman/OMA"></a>This site is maintained by <a href="https://github.com/EliEhrman">EliEhrman</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
