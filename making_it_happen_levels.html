<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>OMA: Making It Happen: Levels of Moral Specification</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1> <a href="index.html" style="color: #fff;">OMA</a></h1>
        <h2>Open Moral Agent</h2>
        <a href="https://github.com/EliEhrman/OMA" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">

<h3>
<a id="Levels of Moral Specification" class="anchor" href="#Levels of Moral Specification" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Levels of Moral Specification</h3>
<p> 
In the section on the core concepts, the idea of open, fully accessible, moral systems was introduced. <br><br> 
The core moral directives for an OMA will be written in plain natural language (such as English, Spanish, Chinese, etc.). Whatever the AMA learns or does, these initial moral directives must never be overridden.<br><br> 
With that in place, the OMA will learn about the world and enrich the understanding of the rules, how they apply and what causal factors advance the goals in the explicit moral directives and what actions endanger them.<br><br> 
Why should some rules be written down and some learnt. Which goes where?<br><br> 
There is far too much detail required to make complete moral evaluations. It would be impossible to encapsulate all the details of these moral directives in text. (Even if we did not subscribe to particularist or related theories of ethics which would not even see it as desirable to be able to do so.) In other words you can't build an OMA by specifying every single moral predicament it might ever face by using words alone. So how is the OMA to be built?<br><br> 
The solution envisioned here is to start with plain-text moral directives. This is not actually a start point since a huge amount of natural language processing capability and knowledge must be built into a system in order to be able to process these plain-text directives.<br><br> 
These plain-text directives allow for a clear, open and accessible evaluation of the starting assumptions and directions of the system.<br><br> 
As a result, what this project seeks to build is not just one OMA instantiation. The open source to be built here is just a framework. In theory, anybody can build any OMA using the framework of this project and start with any set of moral directives. (Yes, there is a potential for abuse here, but so is there in democracy.)<br><br> 
The next stage is to learn. The system aims to fill in the details of the directives using this learning process. It learns the causes a<br><br>nd causal structures that bring about the desirables and proscribed outcomes. By extension, these causes acquire similar moral evaluations as the outcomes.<br><br> 
The observer of the OMA knows the source code of the OMA. He/she also has access to the language knowledge built into the processing of the initial moral directives as well as the text of the directives itself. However, there is a potential for the learning process to significantly undermine what might at first be presumed from knowledge of the source code, NLP capabilities and the directives.<br><br> 
Therefore, the next stage is to allow cross-examination of the post-learning system. This stage allows for direct questions regarding the moral position of the system as it now stands. An important aspect of this is presenting hypothetical situations and asking for moral evaluations based on these hypothetical constructs.<br><br> 
Now, it is possible that an OMA will deceive in its judgements. The explicit directives may allow it or they may at least condone it under certain circumstances. How should the OMA implementor know if the OMA is lying under the cross-examination?<br><br> 
The solution proposed here is to provide different modes of operation. If the OMA is deceiving, it is doing so in fulfillment of the goals of the directives and the learning. Therefore the implementor, or launcher, of the OMA has an initial mode that disables all goal-seeking. Only later, once the implementer and the community that chooses to provide the resources for the OMA, decide that this is the OMA they want, will they let it run in goal-seeking mode. In that goal-seeking mode the OMA applies its moral judgment to deciding what to say when and what actions to perform. <br><br> 
Remember, the code is open source. The part that may be opaque may be the non-source-code knowledge-bases that the OMA creates. The top level that drives the activity of the OMA is open. If the executable is not directed to act upon its knowledge in a goal-seeking way, there is nothing that is going to make it do so. Just looking at the source code should make sure that the OMA has not entered goal-seeking mode yet.<br><br> 
Is the above necessarily correct or flawed in its optimism? The goal of having an open community of developers to build  the OMA framework is to have many eyes scrutinize this and any other assumptions. There may be a flaw in the reasoning, but the goal is to create a significant community of professionals from diverse fields committed to the importance of this mission. Such a community together with the openness proposed here, should provide the best hope to uncovering error.<br><br> 
         </section>

        <aside id="sidebar">
 
          <p class="repo-owner"><a href="https://github.com/EliEhrman/OMA"></a>This site is maintained by <a href="https://github.com/EliEhrman">EliEhrman</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
