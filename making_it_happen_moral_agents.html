<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>OMA: Making It Happen: Moral Agents</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1> <a href="index.html" style="color: #fff;">OMA</a></h1>
        <h2>Open Moral Agent</h2>
        <a href="https://github.com/EliEhrman/OMA" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">

<h3>
<a id="Moral Agents" class="anchor" href="#Moral Agents" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Moral Agents</h3>
<p> 
The term “moral agent” comes from the world of the philosophy of ethics. It is applied in that field, in theory, to any intelligent being. It usually refers to human beings. However, in the context here it is being used to discuss the agency of artificial intelligences only; the software, program, computer or robot.<br><br> 
An agent is something that can act in the world. An agent can do things such as communicate with others, push buttons, break things or create things. The world in which the agent acts is assumed to mean our world; the real world. However, for the purposes of this discussion, all worlds, including virtual worlds will be included.<br><br> 
The philosophical discussion of moral agency often focuses on whether the agent can be held morally responsible for its actions. This obviously implies some level of autonomy as part of the agency. However, the discussion often focuses on Free Will. This is a difficult subject. It seems difficult to prove that even human beings have free will. The very meaning of the term proves elusive.<br><br> 
Nevertheless, moral agency, as used here, need not get into these difficult issues. A system as described in the section called "Moral Assessment", can produce a sentence that we would understand as making a moral evaluation. If a computer were to communicate that sentence to someone, it would be acting in the world. It could be changing the world. It might produce a different world than had it not communicated the sentence containing the moral evaluation.<br><br> 
For example, imagine a chat program that Joan communicates with on a regular basis. The subjects discussed are the people in Joan’s life and her interactions with them. Imagine the moral evaluation is that it would be wrong for Joan to send a hurtful and insulting email to someone who just upset her. As a result of this communication, Joan, who is determined to be a moral person herself, chooses not to send the email. There can be no doubt that the hypothetical situation in which the evaluation was not delivered to Joan is different from the actual situation where it did. It would seem that the chat program (chatbot) is an agent in the world.<br><br> 
There are many other ways in which moral evaluation can result in agency. The program might discover that Joan cheats on her taxes. Assume the program makes the decision to betray her trust and communicates this information to the tax authorities. Is this a moral action? The point is that the computer program would be a moral agent or perhaps an immoral agent. <br><br> 
The program would need to describe the situation, including itself, as an agent, or actor. It would then need to produce a moral evaluation in the form of a text string. Finally, it would need to be able to apply the evaluation to its own actions and thus decide whether to send the information to the tax authorities or not. This three-stage process involves more action-in-the-world than just a text output based on text input.<br><br> 
Finally, this same three-stage process would apply to non-verbal agency such as, say, deciding to disable all the power in a building in the case of some emergency. The situation must be translated into a verbal description. A textual evaluation is created. The textual evaluation is applied to the real-world action.<br><br> 
However, this description of agency can be challenged. One might argue that the program, with its theoretically predictable results is not really autonomous enough to be described as an agent. <br><br> 
To exemplify this challenge one could ask what the difference is between a chatbot that ultimately has been programmed with determinate paths of behavior and a bomb. A bomb also acts on the world. If you press a button it will create a different world to one in which the button was not pressed.<br><br> 
This challenge could be dismissed as semantic. There clearly is a difference, at least one of degree, between the sophisticated agent described here and a one-button bomb. This is, after all, not a philosophical discussion but a practical problem. The explicit goal here is to find mechanisms whereby such an agent can be built and methods must be found for it to perform the actions that the consensus has decided are moral actions.<br><br> 
However, before dismissing this challenge as philosophical, it is worth spending a few words on dealing with it on its own grounds; the philosophical analysis. After all, the philosophy of ethics will have much input that will guide this task. While it is necessary to delineate the aspects of the historical philosophical discussions that are relevant to the task, they are far too valuable to ever be dismissed out of hand.<br><br> 
The difference between the autonomy of the program described here and a bomb is not only one of degree. A definition of the requirements of autonomy can be attempted.<br><br> 
The action of a one-button bomb can be predicted exactly in all scenarios in which it may be deployed. Of course, not all of the scenarios will be known in advance, but one can describe exactly what elements of the scenario play no part in the bomb exploding and therefore all other elements need not be considered in the analysis of whether it will go off. (This is different from predicting the effects, which are not being discussed here).Since we only need to consider the bomb's button in the scenarios, we can say that we can know in advance all the scenarios.<br><br> 
However, for a sophisticated moral agent, any of the elements of the scenario might be a valid part of the decision regarding which action will be taken. Very quickly, it is no longer practically possible to predict what the program will do. This must be used as the definition of the autonomy of the program.<br><br> 
To summarize, the difference between the autonomy of a one-button bomb and a sophisticated program is as follows. For the bomb, predicting its action is limited to whether the button is pressed or not. If the button is pressed, the bomb explodes. All other factors, complex or unpredictable as they may be, are irrelevant. Prediction is therefore complete. <br><br> 
For a complex, sophisticated program based on advance NLP, the factors involved are very many and intricately related to each other. Moreover, all the factors are relevant to predicting the outcome. Therefore, prediction is practically impossible. Lack of simple predictability is certainly one necessary requirement for autonomy.<br><br> 
There is something wrong here. Was not one of the first goals of designing the moral agent that its future actions be predictable? Here one must understand that the predictability in question is different from the predictability of a one-button bomb. The predictability here is that overall the actions taken by the software will further the goals of the task. It is not possible to predict all the details of how exactly how the computer will react every second. That is not expected here. What is expected is that the actions taken by the computer will be in line with the overall goals.<br><br> 
Take an example of mid-level sophistication that can already be spoken of as autonomous. Assume you have an off-road vehicle that you would like to reach a target 10 km away. You could create a remote-control vehicle with a camera. The camera sends the scene from the front of the car back to the base-station operator. Every turn of the wheel or pressing of the gas or brakes is directly activated by the operator. This vehicle is not autonomous.<br><br> 
Assume instead an off-road vehicle that has its camera input fed directly into a computer on board the vehicle. The computer is running sophisticated AI that can recognize the smallest of obstacles and rocks. It can steer the wheels to cross over or swerve around or otherwise navigate the terrain. The vehicle also has a GPS. Now the operator simply enters the target location and lets the car go. This vehicle is not autonomous in the sense that it does not decide where to go to. However, it makes all the decisions required on a millisecond by millisecond basis. It would be very difficult to predict exactly how much acceleration or steering it would produce in each scenario. In fact, it would be difficult to even describe every detail of each scenario in analysis before the fact. The programmer might only specify general rules for how to steer. One could even imagine that the steering may be programmed using machine learning. In this scenario, even if the details cannot be predicted, what can be predicted is that in general the car will move forwards towards its goal.<br><br> 
An autonomous system is defined here as a complex system that makes essentially unpredictable decisions based on highly varied inputs. These decisions are made without external control in real time. <br><br>
        </section>

        <aside id="sidebar">
 
          <p class="repo-owner"><a href="https://github.com/EliEhrman/OMA"></a>This site is maintained by <a href="https://github.com/EliEhrman">EliEhrman</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
