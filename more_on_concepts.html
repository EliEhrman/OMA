<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>OMA: More on the Core Concepts</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1> <a href="index.html" style="color: #fff;">OMA</a></h1>
        <h2>Open Moral Agent</h2>
        <a href="https://github.com/EliEhrman/OMA" class="button"><small>View project on</small> GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">

<h3>
<a id="Moral Software" class="anchor" href="#Moral Software" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Moral Software</h3>
<p> 
We need software that can tell the difference between right and wrong. Computers that run such software should be guided by moral principles such that the actions they take are good rather than evil. 
<br><br>
Of course being good means not hurting people, or, in a positive sense, trying to prevent deaths and suffering. However moral behavior also includes not lying, not willfully deceiving, not violating people’s privacy and keeping promises. 
<br><br>
If our software was moral, we would not need to be afraid of it turning against us. If it does not reveal the information it has gathered about us – assuming that we are not one of the one-in-a-million who wants to hurt innocent people, then we can resolve the tension between privacy and security that so far seems to give us neither. If there is software that keeps its word, then it can act as a fair arbiter between one human and another, but perhaps more importantly between us and other software whose intentions we know less about.  </p>
<h3>

<a id="Trust" class="anchor" href="#Trust" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Trust</h3>
<p> 
A crucial part of this equation is that it is not enough that the software is moral. We need to know that it is moral. The name of the game is trust. There must be a mechanism in place that gives us all the reason in the world to believe that the software is moral and not just some piece of code pretending to act as if it cares about being good. On a more subtle note, even if we can be persuaded that there is some software that has achieved morality, in or oh-so-anonymous Internet, how can we know that it is that software whose moral credentials we have accepted that is the one talking to us at this time? 
<br><br>
In short, we not only need there to be moral software, we need a mechanism in place such that we can trust that the software we are in communication with or is doing some stuff for us or to us, is, in fact, good. </p>
<h3>


<a id="Open" class="anchor" href="#Open" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Open</h3>
<p> 
One of the best ways of knowing what a program does is to read its source code. The source code, the code that is compiled and executed in order to actually run the program is as close to an English definition of what makes the program work on the inside – as you’re going to get. It comprises the logical steps, one stage at a time, that specifies the internal working of the program.  
<br><br>
Programs that are “open source” are programs whose source code is available for anyone to read. If you have the source code, in theory, you will know exactly what the program does. Therefore, in theory, if we want to make sure that our moral software is exactly what it claims to be, we simply have to make a requirement that it be open source. 
<br><br>
However, theory and practice are not always the same thing. Even the programmer who writes the source code may be surprised at the actual behavior of the program. In fact, it is more common than not for the programmer to experience these surprises. These surprises are usually called “bugs”. It takes a tremendous effort to actually work through all the logical implications of the source code until a level of confidence is reached in predicting the behavior of the program. Moreover, only seriously experienced programmers are capable of deciphering the code and all its implications – even for relatively simple programs. As we’ll soon show, the kind of program that can actually make decisions complex enough for us to care whether they’re moral or not will have to be very far from simple.  
<br><br>
There are therefore two problems. One is the level of effort required to determine that even an open source program will behave morally. The other is that we would need to rely on very specific class of professionals to do the evaluation for us.  
<br><br>
What is proposed here is to take a step further in what is meant by open. The instructions for what will constitute the morality of the program must be written in clear English. For example, “Killing is wrong” or “if there is no other way to stop a mass murderer, killing him is right” etc. Even this level of open will not mean that it will be easy to predict what a program will do. However, it will make it open to analysis by a wider audience and with enough effort and careful structuring of the instructions, the goal of moral predictability may be achievable.  
<br><br>
Openness will have to go beyond even this. It must be possible to question or, in fact, cross examine software as to the details of its morality. The software must answer truthfully what it would do or judge under any sort of hypothetical situation one cares to present it with. 
<br><br>
There is another value to this form of openness. There is endless disagreement about the details of what it means to be moral. It will be argued in detail elsewhere that there is no alternative but to allow people to decide for themselves, through a democratic process, decide collectively what they want moral behavior to mean exactly and in detail. Only by making the process truly open will software be moral for each and every person, in the way that they care to define it. 



        </section>

        <aside id="sidebar">
 
          <p class="repo-owner"><a href="https://github.com/EliEhrman/OMA"></a>This site is maintained by <a href="https://github.com/EliEhrman">EliEhrman</a>.</p>

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
